{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cardinality_estimation.featurizer import Featurizer\n",
    "from query_representation.query import load_qrep\n",
    "from cardinality_estimation.dataset import *\n",
    "from torch.utils import data\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-separate",
   "metadata": {},
   "source": [
    "# Setup file paths / Download query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "def make_dir(directory):\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# TRAINDIR = os.path.join(os.path.join(\"\", \"queries\"), \"mlsys1-train\")\n",
    "# VALDIR = os.path.join(os.path.join(\"\", \"queries\"), \"mlsys1-val\")\n",
    "# TESTDIR = os.path.join(os.path.join(\"\", \"queries\"), \"mlsys1-test\")\n",
    "\n",
    "# TRAINDIR = os.path.join(os.path.join(\"\", \"queries\"), \"imdb-unique-plans\")\n",
    "# TESTDIR = os.path.join(os.path.join(\"\", \"queries\"), \"imdb-unique-plans\")\n",
    "\n",
    "TRAINDIR = os.path.join(os.path.join(\"\", \"queries\"), \"imdb\")\n",
    "TESTDIR = os.path.join(os.path.join(\"\", \"queries\"), \"imdb\")\n",
    "\n",
    "RESULTDIR = os.path.join(\"\", \"results\")\n",
    "make_dir(RESULTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-collaboration",
   "metadata": {},
   "source": [
    "# Query loading helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_qdata(fns):\n",
    "    qreps = []\n",
    "    for qfn in fns:\n",
    "        qrep = load_qrep(qfn)\n",
    "        # TODO: can do checks like no queries with zero cardinalities etc.\n",
    "        qreps.append(qrep)\n",
    "        template_name = os.path.basename(os.path.dirname(qfn))\n",
    "        qrep[\"name\"] = os.path.basename(qfn)\n",
    "        qrep[\"template_name\"] = template_name\n",
    "    return qreps\n",
    "\n",
    "def get_query_fns(basedir, template_fraction=1.0, sel_templates=None):\n",
    "    fns = []\n",
    "    tmpnames = list(glob.glob(os.path.join(basedir, \"*\")))\n",
    "    assert template_fraction <= 1.0\n",
    "    \n",
    "    for qi,qdir in enumerate(tmpnames):\n",
    "        if os.path.isfile(qdir):\n",
    "            continue\n",
    "        template_name = os.path.basename(qdir)\n",
    "        if sel_templates is not None and template_name not in sel_templates:\n",
    "            continue\n",
    "        \n",
    "        # let's first select all the qfns we are going to load\n",
    "        qfns = list(glob.glob(os.path.join(qdir, \"*.pkl\")))\n",
    "        qfns.sort()\n",
    "        num_samples = max(int(len(qfns)*template_fraction), 1)\n",
    "        random.seed(1234)\n",
    "        qfns = random.sample(qfns, num_samples)\n",
    "        fns += qfns\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-waterproof",
   "metadata": {},
   "source": [
    "# Evaluation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(alg, qreps):\n",
    "    if isinstance(qreps[0], str):\n",
    "        # only file paths sent\n",
    "        qreps = load_qdata(qreps)\n",
    "    \n",
    "    ests = alg.test(qreps)\n",
    "    return ests\n",
    "\n",
    "def eval_alg(alg, eval_funcs, qreps, samples_type, result_dir=\"./results/\"):\n",
    "    '''\n",
    "    '''\n",
    "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "    alg_name = alg.__str__()\n",
    "    exp_name = alg.get_exp_name()\n",
    "    \n",
    "    if isinstance(qreps[0], str):\n",
    "        # only file paths sent\n",
    "        qreps = load_qdata(qreps)\n",
    "    \n",
    "    ests = alg.test(qreps)\n",
    "\n",
    "    for efunc in eval_funcs:\n",
    "        rdir = None\n",
    "        if result_dir is not None:\n",
    "            rdir = os.path.join(result_dir, exp_name)\n",
    "            make_dir(rdir)\n",
    "\n",
    "        errors = efunc.eval(qreps, ests, samples_type=samples_type,\n",
    "                result_dir=rdir,\n",
    "                num_processes = -1,\n",
    "                alg_name = alg_name,\n",
    "                use_wandb=0)\n",
    "\n",
    "        print(\"{}, {}, #samples: {}, {}: mean: {}, median: {}, 99p: {}\"\\\n",
    "                .format(samples_type, alg, len(errors),\n",
    "                    efunc.__str__(),\n",
    "                    np.round(np.mean(errors),3),\n",
    "                    np.round(np.median(errors),3),\n",
    "                    np.round(np.percentile(errors,99),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-omaha",
   "metadata": {},
   "source": [
    "# Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set template_fraction <= 1.0 to test quickly w/ smaller datasets\n",
    "# train_qfns = get_query_fns(TRAINDIR, template_fraction = 0.001)\n",
    "# val_qfns = get_query_fns(VALDIR, template_fraction = 1.0)\n",
    "# test_qfns = get_query_fns(TESTDIR, template_fraction = 1.0)\n",
    "\n",
    "train_qfns = get_query_fns(TRAINDIR, template_fraction = 0.8, sel_templates=[\"2b\"])\n",
    "val_qfns = []\n",
    "test_qfns = get_query_fns(TESTDIR, template_fraction = 1.0, sel_templates=[\"2a\"])\n",
    "\n",
    "print(\"Selected {} training queries, {} validation queries, {} test queries\".\\\n",
    "      format(len(train_qfns), len(val_qfns), len(test_qfns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.eval_fns import QError, SimplePlanCost\n",
    "EVAL_FNS = []\n",
    "EVAL_FNS.append(QError())\n",
    "EVAL_FNS.append(SimplePlanCost())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_featurizer(featurization_type):\n",
    "    # Load database specific data, e.g., information about columns, tables etc.\n",
    "    dbdata_fn = os.path.join(TRAINDIR, \"dbdata.json\")\n",
    "    featurizer = Featurizer(None, None, None, None, None)\n",
    "    \n",
    "    with open(dbdata_fn, \"r\") as f:\n",
    "        dbdata = json.load(f)\n",
    "        \n",
    "    featurizer.update_using_saved_stats(dbdata)\n",
    "    \n",
    "    featurizer.setup(ynormalization=\"log\",\n",
    "        feat_separate_alias = 0,\n",
    "        onehot_dropout = onehot_dropout,\n",
    "        feat_mcvs = 0,\n",
    "        heuristic_features = 1,\n",
    "        featurization_type=featurization_type,\n",
    "        table_features=1,\n",
    "        flow_features = 0,\n",
    "        join_features= \"onehot\",\n",
    "        set_column_feature= \"onehot\",\n",
    "        max_discrete_featurizing_buckets=10,\n",
    "        max_like_featurizing_buckets=10,\n",
    "        embedding_fn = \"none\",\n",
    "        embedding_pooling = None,\n",
    "        implied_pred_features = 0,\n",
    "        feat_onlyseen_preds = 1)\n",
    "    featurizer.update_ystats(trainqs)\n",
    "    \n",
    "    featurizer.update_max_sets(trainqs)\n",
    "    featurizer.update_workload_stats(trainqs)\n",
    "    featurizer.init_feature_mapping()\n",
    "    #featurizer.update_ystats(trainqs)\n",
    "   \n",
    "\n",
    "    # if feat_onlyseen_preds:\n",
    "    # just do it always\n",
    "    featurizer.update_seen_preds(trainqs)\n",
    "    \n",
    "    return featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734edcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to start training the models\n",
    "trainqs = load_qdata(train_qfns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "testqs = load_qdata(test_qfns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a69fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 30\n",
    "lr=0.0001\n",
    "training_opt = \"none\"\n",
    "opt_lr = 0.1\n",
    "swa_start = 5\n",
    "mask_unseen_subplans = 0\n",
    "subplan_level_outputs=0\n",
    "normalize_flow_loss = 1\n",
    "heuristic_unseen_preds = 0\n",
    "cost_model = \"C\"\n",
    "use_wandb = 0\n",
    "eval_fns = \"qerr,plancost\"\n",
    "load_padded_mscn_feats = 1\n",
    "mb_size = 1024\n",
    "weight_decay = 0.0\n",
    "load_query_together = 0\n",
    "result_dir = \"./results\"\n",
    "\n",
    "onehot_dropout=0\n",
    "onehot_mask_truep=0.8\n",
    "onehot_reg=0\n",
    "onehot_reg_decay=0.1\n",
    "eval_epoch = 200\n",
    "optimizer_name=\"adamw\"\n",
    "clip_gradient=20.0\n",
    "loss_func_name = \"mse\"\n",
    "hidden_layer_size = 128\n",
    "num_hidden_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from cardinality_estimation.mscn import MSCN as MSCN2\n",
    "from cardinality_estimation.mscn import MSCNCaptum as MSCN2\n",
    "\n",
    "featurizer = init_featurizer(\"set\")\n",
    "\n",
    "mscn = MSCN2(max_epochs = max_epochs, lr=lr,\n",
    "                training_opt = training_opt,\n",
    "                inp_dropout = 0.0,\n",
    "                hl_dropout = 0.0,\n",
    "                comb_dropout = 0.0,\n",
    "                max_num_tables = -1,\n",
    "                opt_lr = opt_lr,\n",
    "                swa_start = swa_start,\n",
    "                mask_unseen_subplans = mask_unseen_subplans,\n",
    "                subplan_level_outputs=subplan_level_outputs,\n",
    "                normalize_flow_loss = normalize_flow_loss,\n",
    "                heuristic_unseen_preds = heuristic_unseen_preds,\n",
    "                cost_model = cost_model,\n",
    "                use_wandb = use_wandb,\n",
    "                eval_fns = eval_fns,\n",
    "                load_padded_mscn_feats = load_padded_mscn_feats,\n",
    "                mb_size = mb_size,\n",
    "                weight_decay = weight_decay,\n",
    "                load_query_together = load_query_together,\n",
    "                result_dir = result_dir,\n",
    "                onehot_dropout=onehot_dropout,\n",
    "                onehot_mask_truep=onehot_mask_truep,\n",
    "                onehot_reg=onehot_reg,\n",
    "                onehot_reg_decay=onehot_reg_decay,\n",
    "                # num_hidden_layers=num_hidden_layers,\n",
    "                eval_epoch = eval_epoch,\n",
    "                optimizer_name=optimizer_name,\n",
    "                clip_gradient=clip_gradient,\n",
    "                loss_func_name = loss_func_name,\n",
    "                hidden_layer_size = hidden_layer_size,\n",
    "                other_hid_units = hidden_layer_size,\n",
    "                num_hidden_layers = 2,\n",
    "                early_stopping = False,\n",
    "                random_bitmap_idx = False,\n",
    "                reg_loss = False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c803d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mscn.train(trainqs, valqs=None, testqs=None,\n",
    "    featurizer=featurizer, result_dir=RESULTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "#eval_alg(mscn, EVAL_FNS, trainqs, \"train\")\n",
    "\n",
    "#eval_alg(mscn, EVAL_FNS, valqs, \"val\")\n",
    "# TODO: test set prdictions; should submit these for the leaderboard?\n",
    "#preds = mscn.test(testqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d80e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_alg(mscn, EVAL_FNS, testqs, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62089a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"results/\" + mscn.get_exp_name() + \"/\"\n",
    "print(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from captum library\n",
    "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
    "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation, Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#featurizer = init_featurizer(\"set\")\n",
    "ds = QueryDataset(trainqs[0:10], featurizer,\n",
    "        True,\n",
    "        load_padded_mscn_feats=True)\n",
    "loader = data.DataLoader(ds,\n",
    "        batch_size=1, shuffle=False,\n",
    "        collate_fn=mscn_collate_fn_together,\n",
    "        )\n",
    "\n",
    "testds = QueryDataset(testqs[0:3], featurizer,\n",
    "        True,\n",
    "        load_padded_mscn_feats=True)\n",
    "\n",
    "testloader = data.DataLoader(testds,\n",
    "        batch_size=1, shuffle=False,\n",
    "        collate_fn=mscn_collate_fn_together,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9893dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "iloader = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deffab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbatch,y,info = next(iloader)\n",
    "print(torch.sum(xbatch[\"pred\"][:,:,20]))\n",
    "print(torch.sum(xbatch[\"pred\"][:,:,21]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c518b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "sql = testqs[0][\"sql\"]\n",
    "print(sqlparse.format(sql, reindent=True, keyword_case='upper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3335996",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(featurizer.featurizer_type_idxs[\"col_onehot\"])\n",
    "print(featurizer.columns_onehot_idx)\n",
    "# mii.info\n",
    "# n.name\n",
    "# n.name_pcode_cf\n",
    "# n.surname_pcode\n",
    "#[0  1  6  8  9 11 17 18 20 21 34 37 38 39 40 41 42 43 44 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b090526",
   "metadata": {},
   "outputs": [],
   "source": [
    "curx = xbatch[\"pred\"].detach().numpy()\n",
    "print(curx.shape)\n",
    "xsum = curx.sum(axis=0).sum(axis=0)\n",
    "\n",
    "print(xsum.shape)\n",
    "print(xsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb73a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_vecs(curx, curattrs):\n",
    "    #idxs = 0\n",
    "    xsum = curx.sum(axis=0).sum(axis=0)\n",
    "    zero_idxs = xsum == 0\n",
    "    curattrs = curattrs[:,:,~zero_idxs]\n",
    "    idxs = np.array(range(len(xsum)))[~zero_idxs]\n",
    "    \n",
    "    #print(curx)\n",
    "    #print(zero_idxs)\n",
    "    #print(idxs)\n",
    "    \n",
    "    curx = curx[:,:,~zero_idxs]\n",
    "    \n",
    "    assert curx.shape == curattrs.shape\n",
    "    \n",
    "    # TODO: avg based on non-zero elements\n",
    "    \n",
    "    #print(\"attr sum: \", np.sum(curattrs))\n",
    "    curattrs = np.abs(curattrs)\n",
    "    #print(\"attr sum after abs: \", np.sum(curattrs))\n",
    "    \n",
    "    attr_sum = curattrs.sum(axis=0).sum(axis=0)\n",
    "    \n",
    "    assert attr_sum.shape[0] == curx.shape[-1]\n",
    "    \n",
    "    # TODO: do we need this?\n",
    "    #attr_sum = attr_sum / np.linalg.norm(attr_sum, ord=1)\n",
    "    \n",
    "    # TODO: do this or no?\n",
    "    curx_nonz = curx != 0\n",
    "    \n",
    "    xnonzero_sums = curx_nonz.sum(axis=0).sum(axis=0)\n",
    "    \n",
    "    #TODO?\n",
    "    attr_sum = attr_sum / xnonzero_sums\n",
    "    \n",
    "    return idxs, attr_sum\n",
    "\n",
    "def get_mscn_attrs(xbatch, attrs, featurizer, normalize=True):\n",
    "    '''\n",
    "    returns a vector with x-axis names and attribution values;\n",
    "    '''\n",
    "    batchsize = xbatch[\"table\"].shape[0]\n",
    "    assert batchsize == attrs[0].shape[0]\n",
    "    tabidxs, tabattrs = get_attr_vecs(xbatch[\"table\"].detach().numpy(), attrs[0].detach().numpy())\n",
    "    predidxs, predattrs = get_attr_vecs(xbatch[\"pred\"].detach().numpy(), attrs[1].detach().numpy())\n",
    "    print(predidxs)\n",
    "    joinidxs, joinattrs = get_attr_vecs(xbatch[\"join\"].detach().numpy(), attrs[2].detach().numpy())\n",
    "    \n",
    "    # TODO: need to do sample_bitmaps\n",
    "    tablabels = []\n",
    "    for curtabidx in tabidxs:\n",
    "        for tab,tidx in featurizer.table_featurizer.items():\n",
    "            if tidx == curtabidx:\n",
    "                tablabels.append(tab)\n",
    "                break\n",
    "    joinlabels = []\n",
    "    for curjidx in joinidxs:\n",
    "        for join,jidx in featurizer.join_featurizer.items():\n",
    "            found = False\n",
    "            if curjidx == jidx:\n",
    "                joinlabels.append(join)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            joinlabels.append(\"unknown\")\n",
    "    # TODO: join-stats\n",
    "    \n",
    "    predlabels = []\n",
    "    colstart,collen = featurizer.featurizer_type_idxs[\"col_onehot\"]\n",
    "    # TODO: if stats used\n",
    "    #colstatsstart,colstatsend = self.featurizer_type_idxs[\"col_stats\"]\n",
    "    cmp_start,cmplen = featurizer.featurizer_type_idxs[\"cmp_op\"]\n",
    "    cstart,clen = featurizer.featurizer_type_idxs[\"constant_continuous\"]\n",
    "    lstart,llen = featurizer.featurizer_type_idxs[\"constant_like\"]\n",
    "    dstart,dlen = featurizer.featurizer_type_idxs[\"constant_discrete\"]\n",
    "    hstart,hlen = featurizer.featurizer_type_idxs[\"heuristic_ests\"]\n",
    "    \n",
    "    #print(hstart, hlen)\n",
    "    for pi in predidxs:\n",
    "        if pi >= colstart and pi < colstart+collen:\n",
    "            found = False\n",
    "            for col,colidx in featurizer.columns_onehot_idx.items():\n",
    "                if colidx == pi-colstart:\n",
    "                #if colidx == pi:\n",
    "                    print(col)\n",
    "                    predlabels.append(col)\n",
    "                    found = True\n",
    "                    break     \n",
    "            if not found:\n",
    "                print(pi)\n",
    "                predlabels.append(\"col-unknown\")\n",
    "        elif pi >= cmp_start and pi < cmp_start+cmplen:\n",
    "            predlabels.append(\"cmp\")\n",
    "        elif pi == cstart:\n",
    "            #predlabels.append(\"<\")\n",
    "            predlabels.append(\"range-filter\")\n",
    "        elif pi == cstart+1:\n",
    "            predlabels.append(\"range-filter\")\n",
    "        elif pi >= lstart and pi < lstart+llen:\n",
    "            #predlabels.append(\"Like-Hash-\" + str(pi))\n",
    "            predlabels.append(\"Like-Hashes\")\n",
    "        elif pi >= dstart and pi < dstart+dlen:\n",
    "            #predlabels.append(\"Constant-Hash-\" + str(pi))\n",
    "            predlabels.append(\"Constant-Hashes\")\n",
    "        elif pi == hstart:\n",
    "            predlabels.append(\"PostgreSQL Est (Table)\")\n",
    "        elif pi == hstart+1:\n",
    "            predlabels.append(\"PostgreSQL Est (Subplan)\")\n",
    "    \n",
    "    assert len(predidxs) == len(predlabels)\n",
    "#     print(len(predidxs), len(predlabels))\n",
    "#     print(predidxs)\n",
    "#     print(predlabels)\n",
    "    attrs = np.concatenate([tabattrs, joinattrs, predattrs])\n",
    "    xlabels = tablabels + joinlabels + predlabels\n",
    "    \n",
    "    if normalize:\n",
    "        attrs = attrs / np.linalg.norm(attrs, ord=1)\n",
    "    \n",
    "    return xlabels,attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fe157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardinality_estimation.nets import *\n",
    "\n",
    "def get_model_attrs(mscn_model, weights, xbatch):\n",
    "    n_out = 1\n",
    "    sfeats = mscn_model.sample_mlp1.in_features\n",
    "    pfeats = mscn_model.predicate_mlp1.in_features\n",
    "    jfeats = mscn_model.join_mlp1.in_features\n",
    "\n",
    "    net = SetConvNoFlow(sfeats,\n",
    "        pfeats, jfeats,\n",
    "        128,\n",
    "        n_out=n_out,\n",
    "        dropouts=[0.0, 0.0, 0.0])\n",
    "    net.load_state_dict(weights)\n",
    "    \n",
    "    model = net\n",
    "    model.eval()\n",
    "    ig = IntegratedGradients(model)\n",
    "    #ig_nt = NoiseTunnel(ig)\n",
    "    #dl = DeepLift(model)\n",
    "    #gs = GradientShap(model)\n",
    "    #fa = FeatureAblation(model)\n",
    "    #limea = Lime(model)\n",
    "    ig_attr_test = ig.attribute(tuple([xbatch[\"table\"], xbatch[\"pred\"],\n",
    "                            xbatch[\"join\"], \n",
    "                            xbatch[\"tmask\"], xbatch[\"pmask\"], \n",
    "                                   xbatch[\"jmask\"]]), n_steps=50)\n",
    "    #print(\"ig done\")\n",
    "    \n",
    "    xlabels, igattrs = get_mscn_attrs(xbatch, ig_attr_test, featurizer, normalize=False)\n",
    "\n",
    "    return xlabels, igattrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01413c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e29c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = mscn.net.state_dict()\n",
    "xlabels,attrs = get_model_attrs(mscn.net, weights, xbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mscn.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c943b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_attrs(xlabels, attrs, ax=None):\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        ax = plt.axes()\n",
    "\n",
    "    #plt.yticks(fontsize=20)\n",
    "    sns.barplot(x=attrs, y=xlabels, color='#4260f5', orient=\"horizontal\", ax=ax, ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f3df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# if onehot_dropout:\n",
    "#     with open(\"dropout-attrs.pkl\", \"wb\") as f:\n",
    "#         pickle.dump([xlabels, attrs], f)\n",
    "# else:\n",
    "#     with open(\"default-attrs.pkl\", \"wb\") as f:\n",
    "#         pickle.dump([xlabels, attrs], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attrs(xlabels, attrs)\n",
    "plt.yticks(fontsize=20)\n",
    "#plt.show()\n",
    "plt.savefig(\"attribution-dropout.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207b9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfunc(x):\n",
    "    x = x[x.find(\"-\")+1:x.find(\".ckpt\")]\n",
    "    return int(x)\n",
    "\n",
    "model_fns = glob.glob(models_path + \"*.ckpt\")\n",
    "model_fns.sort(key=sfunc)\n",
    "model_fns\n",
    "\n",
    "NCOLS = 4\n",
    "fig, axs = plt.subplots(nrows=3,ncols=NCOLS, figsize=(40,20))\n",
    "\n",
    "for fi, fn in enumerate(model_fns):\n",
    "    if fi >= 12:\n",
    "        continue\n",
    "    weights = torch.load(fn)\n",
    "    #print(fn)\n",
    "    xlabels,attrs = get_model_attrs(mscn.net, weights, xbatch)\n",
    "    row = int(fi / NCOLS)\n",
    "    col = fi % NCOLS\n",
    "    #print(fi, row, col)\n",
    "    ax = axs[row,col]\n",
    "    plot_attrs(xlabels, attrs, ax=ax)\n",
    "    ax.set_title(str(fi))\n",
    "    \n",
    "    if col != 0:\n",
    "        #ax.set_ylabel(\"\")\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.featurizer_type_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xbatch[\"table\"].sum(axis=[0,1]))\n",
    "print(xbatch[\"pred\"].sum(axis=[0,1]))\n",
    "print(xbatch[\"join\"].sum(axis=[0,1]))\n",
    "\n",
    "print(xbatch[\"table\"].shape)\n",
    "print(xbatch[\"pred\"].shape)\n",
    "print(xbatch[\"join\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53809304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_vecs_single(curx, curattrs):\n",
    "    #idxs = 0\n",
    "    #print(curx.shape)\n",
    "    #print(curattrs.shape)\n",
    "    xsum = curx.sum(axis=0).sum(axis=0)\n",
    "    #print(xsum.shape)\n",
    "    zero_idxs = xsum == 0\n",
    "    curattrs = curattrs[:,:,~zero_idxs]\n",
    "    idxs = np.array(range(len(xsum)))[~zero_idxs]\n",
    "    \n",
    "    curx = curx[:,:,~zero_idxs]\n",
    "    \n",
    "    assert curx.shape == curattrs.shape\n",
    "    \n",
    "    # TODO: abs values or also accept negative correlations?\n",
    "    curattrs = np.abs(curattrs)\n",
    "    #print(curattrs)\n",
    "    \n",
    "    attr_sum = curattrs.sum(axis=0).sum(axis=0)\n",
    "    \n",
    "    assert attr_sum.shape[0] == curx.shape[-1]\n",
    "    \n",
    "    # TODO: do we need this?\n",
    "    #attr_sum = attr_sum / np.linalg.norm(attr_sum, ord=1)\n",
    "    \n",
    "    # TODO: do this or no?\n",
    "    curx_nonz = curx != 0\n",
    "    \n",
    "    xnonzero_sums = curx_nonz.sum(axis=0).sum(axis=0)\n",
    "    \n",
    "#     print(attr_sum)\n",
    "#     print(xnonzero_sums)\n",
    "    # Do this because different features have different number of copies in the same set, \n",
    "    # e.g, subplan features are in every vector\n",
    "    attr_sum = attr_sum / xnonzero_sums\n",
    "    \n",
    "    return idxs, attr_sum\n",
    "\n",
    "def get_mscn_attrs_single(xbatch, xi, attrs, featurizer, normalize=True):\n",
    "    '''\n",
    "    returns a vector with x-axis names and attribution values;\n",
    "    '''\n",
    "    #batchsize = xbatch[\"table\"].shape[0]\n",
    "    #assert batchsize == attrs[0].shape[0]\n",
    "    tabidxs, tabattrs = get_attr_vecs_single(xbatch[\"table\"][xi:xi+1].detach().numpy(), \n",
    "                                      attrs[0].detach().numpy())\n",
    "    predidxs, predattrs = get_attr_vecs_single(xbatch[\"pred\"][xi:xi+1].detach().numpy(), \n",
    "                                        attrs[1].detach().numpy())\n",
    "    #print(predidxs)\n",
    "    joinidxs, joinattrs = get_attr_vecs_single(xbatch[\"join\"][xi:xi+1].detach().numpy(), \n",
    "                                        attrs[2].detach().numpy())\n",
    "    \n",
    "    # TODO: need to do sample_bitmaps\n",
    "    tablabels = []\n",
    "    for curtabidx in tabidxs:\n",
    "        for tab,tidx in featurizer.table_featurizer.items():\n",
    "            if tidx == curtabidx:\n",
    "                tablabels.append(tab)\n",
    "                break\n",
    "    \n",
    "    joinlabels = []\n",
    "    for curjidx in joinidxs:\n",
    "        for join,jidx in featurizer.join_featurizer.items():\n",
    "            found = False\n",
    "            if curjidx == jidx:\n",
    "                joinlabels.append(join)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            joinlabels.append(\"unknown\")\n",
    "    # TODO: join-stats\n",
    "    \n",
    "    predlabels = []\n",
    "    colstart,collen = featurizer.featurizer_type_idxs[\"col_onehot\"]\n",
    "    # TODO: if stats used\n",
    "    #colstatsstart,colstatsend = self.featurizer_type_idxs[\"col_stats\"]\n",
    "    cmp_start,cmplen = featurizer.featurizer_type_idxs[\"cmp_op\"]\n",
    "    cstart,clen = featurizer.featurizer_type_idxs[\"constant_continuous\"]\n",
    "    lstart,llen = featurizer.featurizer_type_idxs[\"constant_like\"]\n",
    "    dstart,dlen = featurizer.featurizer_type_idxs[\"constant_discrete\"]\n",
    "    hstart,hlen = featurizer.featurizer_type_idxs[\"heuristic_ests\"]\n",
    "    \n",
    "    #print(hstart, hlen)\n",
    "    for pi in predidxs:\n",
    "        if pi >= colstart and pi < colstart+collen:\n",
    "            found = False\n",
    "            for col,colidx in featurizer.columns_onehot_idx.items():\n",
    "                if colidx == pi-colstart:\n",
    "                #if colidx == pi:\n",
    "                    #print(col)\n",
    "                    predlabels.append(col)\n",
    "                    found = True\n",
    "                    break     \n",
    "            if not found:\n",
    "                #print(pi)\n",
    "                predlabels.append(\"col-unknown\")\n",
    "                \n",
    "        elif pi >= cmp_start and pi < cmp_start+cmplen:\n",
    "            predlabels.append(\"cmp\")\n",
    "        elif pi == cstart:\n",
    "            #predlabels.append(\"<\")\n",
    "            predlabels.append(\"range-filter\")\n",
    "        elif pi == cstart+1:\n",
    "            predlabels.append(\"range-filter\")\n",
    "        elif pi >= lstart and pi < lstart+llen:\n",
    "            #predlabels.append(\"Like-Hash-\" + str(pi))\n",
    "            predlabels.append(\"Like-Hashes\")\n",
    "        elif pi >= dstart and pi < dstart+dlen:\n",
    "            #predlabels.append(\"Constant-Hash-\" + str(pi))\n",
    "            predlabels.append(\"Constant-Hashes\")\n",
    "        elif pi == hstart:\n",
    "            predlabels.append(\"PostgreSQL Est (Table)\")\n",
    "        elif pi == hstart+1:\n",
    "            predlabels.append(\"PostgreSQL Est (Subplan)\")\n",
    "    \n",
    "    assert len(predidxs) == len(predlabels)\n",
    "\n",
    "    attrs = np.concatenate([tabattrs, joinattrs, predattrs])\n",
    "    xlabels = tablabels + joinlabels + predlabels\n",
    "    \n",
    "    if normalize:\n",
    "        #attrs = attrs / np.linalg.norm(attrs, ord=2)\n",
    "        attrs = attrs / np.sum(attrs)\n",
    "        #print(np.sum(attrs))\n",
    "    \n",
    "    return xlabels,attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597563ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 1\n",
    "sfeats = mscn.net.sample_mlp1.in_features\n",
    "pfeats = mscn.net.predicate_mlp1.in_features\n",
    "jfeats = mscn.net.join_mlp1.in_features\n",
    "    \n",
    "net = SetConvNoFlow(sfeats,\n",
    "    pfeats, jfeats,\n",
    "    128,\n",
    "    n_out=1,\n",
    "    dropouts=[0.0, 0.0, 0.0])\n",
    "net.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7971e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "qrep = testqs[0]\n",
    "subsetg = qrep[\"subset_graph\"]\n",
    "node_list = list(subsetg.nodes())\n",
    "node_list.sort()\n",
    "\n",
    "sfeats = mscn.net.sample_mlp1.in_features\n",
    "pfeats = mscn.net.predicate_mlp1.in_features\n",
    "jfeats = mscn.net.join_mlp1.in_features\n",
    "\n",
    "\n",
    "model = net\n",
    "model.eval()\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "assert xbatch[\"table\"].shape[0] == len(node_list)\n",
    "\n",
    "allsqls = []\n",
    "allxlabels = []\n",
    "alligattrs = []\n",
    "\n",
    "for xi in range(xbatch[\"table\"].shape[0]):\n",
    "    subjg = qrep[\"join_graph\"].subgraph(node_list[xi])\n",
    "    subsql = nx_graph_to_query(subjg)\n",
    "    #print(subsql)\n",
    "    \n",
    "    ig_attr_test = ig.attribute(tuple([xbatch[\"table\"][xi:xi+1], xbatch[\"pred\"][xi:xi+1],\n",
    "                            xbatch[\"join\"][xi:xi+1], \n",
    "                            xbatch[\"tmask\"][xi:xi+1], xbatch[\"pmask\"][xi:xi+1], \n",
    "                                   xbatch[\"jmask\"][xi:xi+1]]), n_steps=50)\n",
    "\n",
    "    #print(\"ig done\")\n",
    "    xlabels, igattrs = get_mscn_attrs_single(xbatch, xi, \n",
    "                                ig_attr_test, featurizer, \n",
    "                                             normalize=True)\n",
    "    #print(\"Xlabels: \", xlabels)\n",
    "    #print(igattrs)\n",
    "    #break\n",
    "    allsqls.append(subsql)\n",
    "    allxlabels.append(xlabels)\n",
    "    alligattrs.append(igattrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbatch[\"table\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(allsqls, tables):\n",
    "    for i,sql in enumerate(allsqls):\n",
    "        sql = sql.lower()\n",
    "        num_tabs = len(sql.split(\"as\"))-2\n",
    "        \n",
    "        if num_tabs != len(tables):\n",
    "            continue\n",
    "        \n",
    "        #print(sql)\n",
    "        allinsql = True\n",
    "        for t in tables:\n",
    "            if \"as \" + t not in sql:\n",
    "                allinsql = False\n",
    "                break\n",
    "        if allinsql:\n",
    "            return i\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tables = [\"ci\", \"t\"]\n",
    "tables = [\"ci\", \"t\", \"mk\", \"k\"]\n",
    "idx = find_index(allsqls, tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7763209",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779dabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "allsqls[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5fe537",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels = allxlabels[idx]\n",
    "igattrs = alligattrs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcc9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "igattrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234302d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(igattrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if onehot_dropout:\n",
    "    with open(\"subplan-dropout-attrs.pkl\", \"wb\") as f:\n",
    "        pickle.dump([allsqls, allxlabels, alligattrs], f, protocol=3)\n",
    "else:\n",
    "    print(\"saving default\")\n",
    "    with open(\"subplan-default-attrs.pkl\", \"wb\") as f:\n",
    "        pickle.dump([allsqls, allxlabels, alligattrs], f, protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"subplan-dropout-attrs.pkl\", \"rb\") as f:\n",
    "    dropoutdata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f067b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# cm = plt.cm.get_cmap('RdYlBu')\n",
    "# #xy = range(20)\n",
    "# #z = xy\n",
    "# sc = plt.text(\"SELECT COUNT\\n\", x=0.0, y=0.0, vmin=0, vmax=20, s=35, cmap=cm)\n",
    "# plt.colorbar(cm)\n",
    "# #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc44477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm = plt.cm.get_cmap('RdYlGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72af23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "igattrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "norm = mpl.colors.Normalize(vmin=min(igattrs), vmax=max(igattrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb2c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm(norm(igattrs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad0c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm(norm(igattrs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e703b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsql = allsqls[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels = allxlabels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = subsql.split(\"WHERE\")[0]\n",
    "filters = subsql.split(\"WHERE\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = filters.split(\"AND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee197345",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filters)\n",
    "print(xlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ac724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_idx(xlabels, col):\n",
    "    curidx = None\n",
    "    for xi, xlabel in enumerate(xlabels):\n",
    "        if col == xlabel:\n",
    "            curidx = xi\n",
    "            break\n",
    "    return curidx\n",
    "\n",
    "def color_subsql(subsql, xlabels, scores):\n",
    "    \n",
    "    fstrs = []\n",
    "    tstrs = []\n",
    "    attrvals = []\n",
    "    \n",
    "    tables = subsql.split(\"WHERE\")[0]\n",
    "    tables = tables[tables.find(\"FROM\")+5:]\n",
    "    tables = tables.split(\",\")\n",
    "\n",
    "    for t in tables:\n",
    "        tab = t.split(\"AS\")[0]\n",
    "        alias = t.split(\"AS\")[1]\n",
    "        tab = tab.replace(\" \", \"\")\n",
    "        alias = alias.replace(\" \", \"\")\n",
    "        \n",
    "        curidx = None\n",
    "        for xi, xlabel in enumerate(xlabels):\n",
    "            if tab == xlabel:\n",
    "                curidx = xi\n",
    "        assert curidx is not None\n",
    "        \n",
    "        tab = \"{} AS {}\".format(tab, alias)\n",
    "        tstrs.append(\"<\" + tab + \">\")\n",
    "        attrvals.append(scores[curidx])\n",
    "    \n",
    "    filters = subsql.split(\"WHERE\")[1]\n",
    "    filters = filters.split(\"AND\")\n",
    "    \n",
    "    for f in filters:\n",
    "        f = f.replace(\" \", \"\")\n",
    "        # find which idx in xlabel belongs to it\n",
    "        \n",
    "        ## more careful with the filters\n",
    "        if \"<=\" in f:\n",
    "            fcol = f.split(\"<=\")[0][1:]\n",
    "            fval = f.split(\"<=\")[1][:-1]\n",
    "            #print(fcol)\n",
    "            colidx = get_col_idx(xlabels, fcol)\n",
    "            assert colidx is not None\n",
    "            curfstr = \"<\"+fcol+\">\"\n",
    "            curfstr += \" leq \"\n",
    "            curfstr += fval\n",
    "            fstrs.append(curfstr)\n",
    "            attrvals.append(scores[colidx]) \n",
    "        elif \">=\" in f:\n",
    "            fcol = f.split(\">=\")[0][1:]\n",
    "            fval = f.split(\">=\")[1][:-1]\n",
    "            colidx = get_col_idx(xlabels, fcol)\n",
    "            curfstr = \"<\"+fcol+\">\"\n",
    "            curfstr += \" gt \"\n",
    "            curfstr += fval\n",
    "            fstrs.append(curfstr)\n",
    "            attrvals.append(scores[colidx])\n",
    "        elif \"id\" in f:\n",
    "            assert \"=\" in f\n",
    "            left = f.split(\"=\")[0]\n",
    "            leftcol = left[left.find(\".\")+1:]\n",
    "            right = f.split(\"=\")[1]\n",
    "            rightcol = right[right.find(\".\")+1:]\n",
    "            curidx = None\n",
    "            for xi, xlabel in enumerate(xlabels):\n",
    "                if leftcol in xlabel and rightcol in xlabel:\n",
    "                    curidx = xi\n",
    "                    break\n",
    "            assert curidx is not None\n",
    "            fstrs.append(\"<\" + f + \">\")\n",
    "            attrvals.append(scores[xi])\n",
    "            \n",
    "        elif \"in\" in f.lower():\n",
    "            print(\"IN!\")\n",
    "        else:\n",
    "            print(\"Unknown:\")\n",
    "            print(f)\n",
    "    \n",
    "    fscores = attrvals\n",
    "    \n",
    "    final_fmt = \"\"\"SELECT COUNT(*) FROM\\n{TABS}\\nWHERE {FILTERS}\\n\\n<PostgreSQL table estimate>\\n<PostgreSQL subplan estimate>\"\"\"\n",
    "    \n",
    "    fscores.append(scores[-2])\n",
    "    fscores.append(scores[-1])\n",
    "    \n",
    "    fstrs = [f+\"\\n\" for f in fstrs] \n",
    "    tabs = \",\\n\".join(tstrs)\n",
    "    fstr = \"AND \".join(fstrs)\n",
    "    #print(fstr)\n",
    "    final_sql = final_fmt.format(TABS = tabs, FILTERS=fstr)\n",
    "    \n",
    "    return final_sql, fscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql, scores = color_subsql(allsqls[idx], allxlabels[idx], alligattrs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from highlight_text import HighlightText, ax_text, fig_text\n",
    "# or\n",
    "import highlight_text # then use highlight_text.ax_text or highlight_text.fig_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cca1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm = plt.cm.get_cmap('RdYlGn')\n",
    "from bokeh.palettes import Blues256 as PAL\n",
    "\n",
    "#cm = plt.cm.get_cmap('Blues')\n",
    "#cm = PAL\n",
    "#cm = LinearColorMapper(\"test\", PAL)\n",
    "#cm = plt.get_cmap('viridis')\n",
    "cm = plt.get_cmap('Blues')\n",
    "cm = truncate_colormap(cm, 0.4, 1.0)\n",
    "\n",
    "import matplotlib as mpl\n",
    "norm = mpl.colors.Normalize(vmin=min(scores), vmax=max(scores))\n",
    "color_props = [{\"color\":cm(norm(sc)), \"fontweight\":\"normal\"} for sc in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e70e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,6.5))  \n",
    "\n",
    "# You can either create a HighlightText object\n",
    "HighlightText(x=0.0, y=1.0,\n",
    "              s=sql,\n",
    "              #fontweight='normal',\n",
    "              fontsize=22,\n",
    "              highlight_textprops=color_props,\n",
    "              ax=ax)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=cm, norm=norm)\n",
    "#plt.colorbar(sm, orientation=\"vertical\", label=\"Importance\")\n",
    "plt.colorbar(sm, orientation=\"horizontal\", label=\"Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf53473",
   "metadata": {},
   "outputs": [],
   "source": [
    "alligattrs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a822f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
