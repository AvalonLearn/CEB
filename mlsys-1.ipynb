{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "engaged-government",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from query_representation.query import load_qrep\n",
    "from cardinality_estimation.featurizer import Featurizer\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-separate",
   "metadata": {},
   "source": [
    "# Setup file paths / Download query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "perfect-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "def make_dir(directory):\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hidden-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "TRAINDIR = os.path.join(os.path.join(\"\", \"queries\"), \"mlsys1-train\")\n",
    "VALDIR = os.path.join(os.path.join(\"\", \"queries\"), \"mlsys1-val\")\n",
    "TESTDIR = os.path.join(os.path.join(\"\", \"queries\"), \"mlsys1-test\")\n",
    "\n",
    "RESULTDIR = os.path.join(\"\", \"results\")\n",
    "make_dir(RESULTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-collaboration",
   "metadata": {},
   "source": [
    "# Query loading helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hollywood-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qdata(fns):\n",
    "    qreps = []\n",
    "    for qfn in fns:\n",
    "        qrep = load_qrep(qfn)\n",
    "        # TODO: can do checks like no queries with zero cardinalities etc.\n",
    "        qreps.append(qrep)\n",
    "        template_name = os.path.basename(os.path.dirname(qfn))\n",
    "        qrep[\"name\"] = os.path.basename(qfn)\n",
    "        qrep[\"template_name\"] = template_name\n",
    "    return qreps\n",
    "\n",
    "def get_query_fns(basedir, template_fraction=1.0):\n",
    "    fns = []\n",
    "    tmpnames = list(glob.glob(os.path.join(basedir, \"*\")))\n",
    "    assert template_fraction <= 1.0\n",
    "    \n",
    "    for qi,qdir in enumerate(tmpnames):\n",
    "        if os.path.isfile(qdir):\n",
    "            continue\n",
    "        template_name = os.path.basename(qdir)\n",
    "        # let's first select all the qfns we are going to load\n",
    "        qfns = list(glob.glob(os.path.join(qdir, \"*.pkl\")))\n",
    "        qfns.sort()\n",
    "        num_samples = max(int(len(qfns)*template_fraction), 1)\n",
    "        random.seed(1234)\n",
    "        qfns = random.sample(qfns, num_samples)\n",
    "        fns += qfns\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-waterproof",
   "metadata": {},
   "source": [
    "# Evaluation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mineral-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_alg(alg, eval_funcs, qreps, samples_type, result_dir=\"./results/\"):\n",
    "    '''\n",
    "    '''\n",
    "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "    alg_name = alg.__str__()\n",
    "    exp_name = alg.get_exp_name()\n",
    "    ests = alg.test(qreps)\n",
    "\n",
    "    for efunc in eval_funcs:\n",
    "        rdir = None\n",
    "        if result_dir is not None:\n",
    "            rdir = os.path.join(result_dir, exp_name)\n",
    "            make_dir(rdir)\n",
    "\n",
    "        errors = efunc.eval(qreps, ests, samples_type=samples_type,\n",
    "                result_dir=rdir,\n",
    "                num_processes = -1,\n",
    "                alg_name = alg_name)\n",
    "\n",
    "        print(\"{}, {}, #samples: {}, {}: mean: {}, median: {}, 99p: {}\"\\\n",
    "                .format(samples_type, alg, len(errors),\n",
    "                    efunc.__str__(),\n",
    "                    np.round(np.mean(errors),3),\n",
    "                    np.round(np.median(errors),3),\n",
    "                    np.round(np.percentile(errors,99),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-omaha",
   "metadata": {},
   "source": [
    "# Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sacred-arena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1256 training queries, 621 validation queries, 1260 test queries\n"
     ]
    }
   ],
   "source": [
    "# set template_fraction <= 1.0 to test quickly w/ smaller datasets\n",
    "trainqs = load_qdata(get_query_fns(TRAINDIR, template_fraction = 1.0))\n",
    "valqs = load_qdata(get_query_fns(VALDIR, template_fraction = 1.0))\n",
    "testqs = load_qdata(get_query_fns(TESTDIR, template_fraction = 1.0))\n",
    "\n",
    "print(\"Loaded {} training queries, {} validation queries, {} test queries\".\\\n",
    "      format(len(trainqs), len(valqs), len(testqs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-mentor",
   "metadata": {},
   "source": [
    "# Explore Queries (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "alpha-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose particular query, and show its properties + what exactly is cardinality estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-yesterday",
   "metadata": {},
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "informational-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.eval_fns import QError, SimplePlanCost\n",
    "EVAL_FNS = []\n",
    "EVAL_FNS.append(QError())\n",
    "EVAL_FNS.append(SimplePlanCost())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-bearing",
   "metadata": {},
   "source": [
    "# Evaluating baseline / heuristic estimates (TODO)\n",
    "### shows example of true cardinalities, and postgresql estimates; introduces Q-Error and PlanCost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-chapter",
   "metadata": {},
   "source": [
    "# Helper function for initializing featurizer\n",
    "### Featurizer object contains information about the db, e.g., tables, joins, columns, how to featurize predicate filters etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "numeric-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_featurizer(featurization_type):\n",
    "    # Load database specific data, e.g., information about columns, tables etc.\n",
    "    dbdata_fn = os.path.join(TRAINDIR, \"dbdata.json\")\n",
    "    featurizer = Featurizer(None, None, None, None, None)\n",
    "    with open(dbdata_fn, \"r\") as f:\n",
    "        dbdata = json.load(f)\n",
    "    featurizer.update_using_saved_stats(dbdata)\n",
    "\n",
    "    # ynormalization: takes log(y) for all target values, y.\n",
    "    featurizer.setup(ynormalization=\"log\",\n",
    "            featurization_type=featurization_type)\n",
    "    featurizer.update_ystats(trainqs)\n",
    "    return featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "heavy-victoria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queries/mlsys1-train'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-ranking",
   "metadata": {},
   "source": [
    "# RandomForest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "filled-gibson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features took:  93.56864786148071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:   44.7s remaining:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   45.1s finished\n"
     ]
    }
   ],
   "source": [
    "from cardinality_estimation.algs import RandomForest\n",
    "featurizer = init_featurizer(\"combined\")\n",
    "rf = RandomForest(grid_search = False,\n",
    "                n_estimators = 10,\n",
    "                max_depth = 6)\n",
    "rf.train(trainqs, valqs=None, testqs=None,\n",
    "    featurizer=featurizer, result_dir=RESULTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-newport",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name will be:  RandomForest24825937\n",
      "Extracting features took:  95.80298805236816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, RandomForest, #samples: 546873, QError: mean: 133.507, median: 3.925, 99p: 620.755\n",
      "train, RandomForest, #samples: 1256, SimplePlanCost: mean: 3777338.513, median: 981477.66, 99p: 41279502.652\n",
      "Extracting features took:  46.78105711936951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val, RandomForest, #samples: 271741, QError: mean: 226.26, median: 3.97, 99p: 735.144\n",
      "val, RandomForest, #samples: 621, SimplePlanCost: mean: 3149672.976, median: 899305.046, 99p: 30179417.41\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "eval_alg(rf, EVAL_FNS, trainqs, \"train\")\n",
    "eval_alg(rf, EVAL_FNS, valqs, \"val\")\n",
    "\n",
    "# TODO: should submit these for the leaderboard?\n",
    "preds = rf.test(testqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-ranking",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "divine-award",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features took:  22.899195909500122\n"
     ]
    }
   ],
   "source": [
    "from cardinality_estimation.algs import XGBoost\n",
    "featurizer = init_featurizer(\"combined\")\n",
    "xgb = XGBoost(grid_search=False, tree_method=\"hist\",\n",
    "                       subsample=1.0, n_estimators = 100,\n",
    "                       max_depth=10, lr = 0.01)\n",
    "xgb.train(trainqs, valqs=None, testqs=None,\n",
    "    featurizer=featurizer, result_dir=RESULTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "eval_alg(xgb, EVAL_FNS, trainqs, \"train\")\n",
    "eval_alg(xgb, EVAL_FNS, valqs, \"val\")\n",
    "\n",
    "\n",
    "# TODO: test set prdictions; should submit these for the leaderboard?\n",
    "preds = xgb.test(testqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-excess",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "faced-variable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features took:  22.69448685646057\n",
      "SimpleRegression(\n",
      "  (layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=515, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "training samples: 133784, feature length: 515, model size: 1.318916,\n",
      "        hidden_layer_size: 256\n",
      "Epoch 0 took 2.22, Avg Loss: 0.009701\n",
      "Epoch 1 took 2.69, Avg Loss: 0.00216\n",
      "Epoch 2 took 2.37, Avg Loss: 0.001335\n",
      "Epoch 3 took 2.49, Avg Loss: 0.00103\n",
      "Epoch 4 took 2.48, Avg Loss: 0.000851\n",
      "Epoch 5 took 2.46, Avg Loss: 0.00075\n",
      "Epoch 6 took 2.5, Avg Loss: 0.000655\n",
      "Epoch 7 took 2.87, Avg Loss: 0.000591\n",
      "Epoch 8 took 2.34, Avg Loss: 0.000544\n",
      "Epoch 9 took 2.37, Avg Loss: 0.000504\n"
     ]
    }
   ],
   "source": [
    "from cardinality_estimation.fcnn import FCNN\n",
    "featurizer = init_featurizer(\"combined\")\n",
    "fcnn = FCNN(max_epochs = 10,\n",
    "     lr=0.0001,\n",
    "     mb_size = 512,\n",
    "     weight_decay = 0.0,\n",
    "     result_dir = \"./results\",\n",
    "     num_hidden_layers=4,\n",
    "     optimizer_name=\"adamw\",\n",
    "     clip_gradient=20.0,\n",
    "     loss_func_name = \"mse\",\n",
    "     hidden_layer_size = 256)\n",
    "\n",
    "fcnn.train(trainqs, valqs=None, testqs=None,\n",
    "    featurizer=featurizer, result_dir=RESULTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "sweet-pacific",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name will be:  FCNN1017090249\n",
      "Extracting features took:  22.05372905731201\n",
      "train, FCNN, #samples: 133784, QError: mean: 1.834, median: 1.347, 99p: 6.04\n",
      "train, FCNN, #samples: 307, SimplePlanCost: mean: 2868145.107, median: 471279.311, 99p: 48213382.76\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "eval_alg(fcnn, EVAL_FNS, trainqs, \"train\")\n",
    "eval_alg(fcnn, EVAL_FNS, valqs, \"val\")\n",
    "\n",
    "# TODO: test set prdictions; should submit these for the leaderboard?\n",
    "preds = fcnn.test(testqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-execution",
   "metadata": {},
   "source": [
    "# Multi Set Convolutional Network\n",
    "\n",
    "## Notes\n",
    "\n",
    "* Introduced by Kipf et al. in this [paper](https://arxiv.org/abs/1809.00677). Architecture based on [Deep Sets](https://arxiv.org/abs/1703.06114).\n",
    "* Does not reserve an exact mapping for features on a particular table / column. Treats table features, join features, and predicate features as set of vectors. Has practical benefits over the flat 1d featurization (see discussion in README). But requires each batch to have same shape; thus a lot of the smaller query features need to be padded with zeros, which makes the memory consumption become much larger (can probably improve this somehow).\n",
    "* load_padded_mscn_feats = True (see MSCN initialization below), loads these padded sets in memory; takes more RAM, but is faster; load_padded_mscn_feats = False, pads the vectors as needed --- takes longer to train (TODO: current python implementation can be improved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dominican-punishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features took:  27.066265106201172\n",
      "SetConv(\n",
      "  (sample_mlp1): Linear(in_features=15, out_features=256, bias=True)\n",
      "  (sample_mlp2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (predicate_mlp1): Linear(in_features=55, out_features=256, bias=True)\n",
      "  (predicate_mlp2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (join_mlp1): Linear(in_features=43, out_features=256, bias=True)\n",
      "  (join_mlp2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (out_mlp1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (out_mlp2): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "training samples: 133784, model size: 1.696772,\n",
      "        hidden_layer_size: 256\n",
      "Epoch 0 took 90.57, Avg Loss: 0.012292\n",
      "Epoch 1 took 95.94, Avg Loss: 0.005715\n",
      "Epoch 2 took 97.58, Avg Loss: 0.0046\n",
      "Epoch 3 took 93.91, Avg Loss: 0.004018\n",
      "Epoch 4 took 93.38, Avg Loss: 0.003625\n",
      "Epoch 5 took 96.69, Avg Loss: 0.003289\n",
      "Epoch 6 took 91.85, Avg Loss: 0.003012\n",
      "Epoch 7 took 94.12, Avg Loss: 0.002765\n",
      "Epoch 8 took 90.93, Avg Loss: 0.002575\n",
      "Epoch 9 took 93.98, Avg Loss: 0.002387\n"
     ]
    }
   ],
   "source": [
    "from cardinality_estimation.mscn import MSCN\n",
    "\n",
    "featurizer = init_featurizer(\"set\")\n",
    "\n",
    "# load_padded_mscn_feats = True means all the fea\n",
    "mscn = MSCN(max_epochs = 10,\n",
    "     load_padded_mscn_feats = False,\n",
    "     lr=0.0001,\n",
    "     mb_size = 512,\n",
    "     weight_decay = 0.0,\n",
    "     result_dir = \"./results\",\n",
    "     optimizer_name=\"adamw\",\n",
    "     clip_gradient=20.0,\n",
    "     loss_func_name = \"mse\",\n",
    "     hidden_layer_size = 256)\n",
    "\n",
    "mscn.train(trainqs, valqs=None, testqs=None,\n",
    "    featurizer=featurizer, result_dir=RESULTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "random-share",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name will be:  MSCN132345184\n",
      "Extracting features took:  104.61881613731384\n",
      "train, MSCN, #samples: 133784, QError: mean: 37.971, median: 2.293, 99p: 45.606\n",
      "train, MSCN, #samples: 307, SimplePlanCost: mean: 3125948.222, median: 505817.82, 99p: 37967613.474\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "eval_alg(mscn, EVAL_FNS, trainqs, \"train\")\n",
    "eval_alg(mscn, EVAL_FNS, valqs, \"val\")\n",
    "\n",
    "# TODO: test set prdictions; should submit these for the leaderboard?\n",
    "preds = mscn.test(testqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-harvard",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
